{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Regressor Model\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from xgboost import XGBRegressor \n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, KFold, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "features = pd.read_csv(\"./data/prepped/modeling_features.csv\").drop([\"Unnamed: 0\"], axis=1)\n",
    "labels = pd.read_csv(\"./data/prepped/modeling_outcome.csv\").drop([\"Unnamed: 0\"], axis=1)\n",
    "\n",
    "train_features, test_features, train_outcome, test_outcome = train_test_split(\n",
    "    features,\n",
    "    labels,\n",
    "    test_size=0.25,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Model\n",
    "\n",
    "The model used is the Extreme Gradient Boosted Regressor found [here](https://xgboost.readthedocs.io/en/latest/python/python_api.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07748200692578133\n"
     ]
    }
   ],
   "source": [
    "xgb_base = XGBRegressor(nthread=-1, random_state=42) \n",
    "xgb_base.fit(train_features, train_outcome)\n",
    "\n",
    "base_predictions = xgb_base.score(test_features, test_outcome)\n",
    "print(base_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree based feature selection\n",
    "\n",
    "Through domain research and exploratory data analysis we were able to determine that some features are not as important as others. Therefore, here we attempt to reduce the number of features used within our model by extracting features that are more significant that others with Tree-Based Feature Selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.05672566781242483\n",
      "Feature Importance\n",
      "[0.19009584, 0.1086262, 0.052715655, 0.0686901, 0.13258786, 0.08306709, 0.052715655, 0.041533545, 0.07827476, 0.047923323, 0.095846646, 0.047923323]\n",
      "Refined Feature Lists\n",
      "['abv', 'ibu', 'diff_g', 'boil_time', 'efficiency', 'ferm_total_weight', 'ferm_type_base_malt', 'ferm_type_crystal_malt', 'ferm_type_roasted_malt', 'ferm_type_other', 'ferm_type_extract', 'ferm_type_sugar']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "feature_model = SelectFromModel(xgb_base, prefit=True)\n",
    "\n",
    "# Transform into new training and testing feature sets\n",
    "refined_train_features = feature_model.transform(train_features)\n",
    "refined_test_features = feature_model.transform(test_features)\n",
    "\n",
    "# Retrained with refined features\n",
    "xgb_refined = XGBRegressor(nthread=-1, random_state=42)\n",
    "xgb_refined.fit(refined_train_features, train_outcome)\n",
    "\n",
    "print(\"Score: \" + str(xgb_refined.score(refined_test_features, test_outcome)))\n",
    "print(\"Feature Importance\")\n",
    "print(list(xgb_refined.feature_importances_))\n",
    "feature_tuples = [(feature, round(importance, 2)) for feature, importance in zip(list(features.columns), list(xgb_refined.feature_importances_))]\n",
    "refined_features = list(dict(feature_tuples))\n",
    "print(\"Refined Feature Lists\")\n",
    "print(refined_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation\n",
    "\n",
    "Cross validation was attempted for a range of parameters within our model. After reading more into RandomForestClassifier, I concluded on five parameters to tune in order to build a better model. Those parameters included that of n_estimators, max_features, max_depth, min_samples_split and bootstrap. These all are important as they allow us to control the number of trees in the forest as well as the number of features considered for splitting at each leaf node. The values derived here are a result of my intial model with variations to the range of potential values in order to attempt to capture a better fit within the range of parameter values.\n",
    "\n",
    "Parameters for the model can be found [here]( https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst#parameters-for-linear-booster-booster-gblinear)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   13.9s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  3.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10205588301716784\n"
     ]
    }
   ],
   "source": [
    "# Number of trees\n",
    "n_estimators = [int(x) for x in np.linspace(start = 300, stop = 800, num = 10)]\n",
    "\n",
    "# Maximum number of levels in tree [0, inf]\n",
    "max_depth = [int(x) for x in np.linspace(80, 120, num = 10)]\n",
    "\n",
    "# Minimum number of samples required to split a node [0, 1]\n",
    "learning_rate = [.009, .01, .09] #[i/10.0 for i in range(1,11)]\n",
    "\n",
    "# Minimum loss reduction required to make a further partition on a leaf node of the tree [0, inf]\n",
    "gamma = [.27, .3, .34] #[i/10.0 for i in range(3,6)]\n",
    "\n",
    "# Subsample ratio of the training instances. (0,1]\n",
    "subsample = [i/10.0 for i in range(6,11)]\n",
    "\n",
    "# Subsample ratio of columns when constructing each tree. (0,1]\n",
    "colsample_bytree = [.35, .4, .45] #[i/10.0 for i in range(1,8)]\n",
    "    \n",
    "# Minimum sum of instance weight (hessian) needed in a child. [0,∞]\n",
    "min_child_weight = [.1, .15, .2, .25, 1]\n",
    "\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {\n",
    "        'objective':['reg:linear'],\n",
    "        'learning_rate': learning_rate,\n",
    "        'min_child_weight': min_child_weight,\n",
    "        'gamma': gamma,\n",
    "        'subsample': subsample,\n",
    "        'silent': [1],\n",
    "        'colsample_bytree': colsample_bytree,\n",
    "        'max_depth': max_depth,\n",
    "        'n_estimators': n_estimators\n",
    "        }\n",
    "\n",
    "cv_model = RandomizedSearchCV(estimator=XGBRegressor(nthread=-1), param_distributions=random_grid,\n",
    "                             n_iter = 100, scoring='r2', \n",
    "                              cv = 5, verbose=True, random_state=42, n_jobs=-1)\n",
    "\n",
    "cv_model.fit(refined_train_features, train_outcome)\n",
    "\n",
    "print(cv_model.score(refined_test_features, test_outcome))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_model.best_params_\n",
    "\n",
    "# cv_model.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 5832 candidates, totalling 58320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   17.2s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  6.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed: 10.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1784 tasks      | elapsed: 16.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2434 tasks      | elapsed: 22.9min\n",
      "[Parallel(n_jobs=-1)]: Done 3184 tasks      | elapsed: 29.0min\n",
      "[Parallel(n_jobs=-1)]: Done 4034 tasks      | elapsed: 37.8min\n",
      "[Parallel(n_jobs=-1)]: Done 4984 tasks      | elapsed: 47.0min\n",
      "[Parallel(n_jobs=-1)]: Done 6034 tasks      | elapsed: 57.4min\n",
      "[Parallel(n_jobs=-1)]: Done 7184 tasks      | elapsed: 69.3min\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-b4632ba8cfae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipeline_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfolds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_outcome\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_outcome\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    638\u001b[0m                                   error_score=self.error_score)\n\u001b[1;32m    639\u001b[0m           for parameters, (train, test) in product(candidate_params,\n\u001b[0;32m--> 640\u001b[0;31m                                                    cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0;31m# if one choose to see train score, \"out\" will contain train score info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    697\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "folds = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    MinMaxScaler(),\n",
    "    SelectPercentile(),\n",
    "    XGBRegressor(nthread=-1)\n",
    ")\n",
    "\n",
    "\n",
    "# Number of trees\n",
    "n_estimators = [500, 530, 560]\n",
    "\n",
    "# Maximum number of levels in tree [0, inf]\n",
    "max_depth = [85, 93, 100]\n",
    "\n",
    "# Minimum number of samples required to split a node [0, 1]\n",
    "learning_rate = [.009, .01, .09]\n",
    "\n",
    "# Minimum loss reduction required to make a further partition on a leaf node of the tree [0, inf]\n",
    "gamma = [.3, .35, .4]\n",
    "\n",
    "# Subsample ratio of the training instances. (0,1]\n",
    "subsample = [.9, 1.0]\n",
    "\n",
    "# Subsample ratio of columns when constructing each tree. (0,1]\n",
    "colsample_bytree = [.35, .4, .45]\n",
    "    \n",
    "# Minimum sum of instance weight (hessian) needed in a child. [0,∞]\n",
    "min_child_weight = [.05 ,.1, .15, .2]\n",
    "\n",
    "\n",
    "pipeline_params = {\n",
    "    \"selectpercentile__percentile\": [70, 80, 90],\n",
    "    \"xgbregressor__n_estimators\": n_estimators,\n",
    "    \"xgbregressor__subsample\": subsample,\n",
    "    \"xgbregressor__gamma\": gamma,\n",
    "    \"xgbregressor__learning_rate\": learning_rate,\n",
    "    \"xgbregressor__max_depth\": max_depth,\n",
    "    \"xgbregressor__colsample_bytree\": colsample_bytree,\n",
    "    \"xgbregressor__min_child_weight\": min_child_weight,\n",
    "}\n",
    "\n",
    "model = GridSearchCV(pipeline, pipeline_params, cv=folds, n_jobs=-1, verbose=True)\n",
    "model.fit(train_features, train_outcome)\n",
    "score = model.score(test_features, test_outcome)\n",
    "\n",
    "print(\"model score:\", score)\n",
    "print(\"best params:\", model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
